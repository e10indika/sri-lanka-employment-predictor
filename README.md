# Sri Lanka Employment Predictor

A modular machine learning application for predicting employment status with support for multiple ML models, comprehensive model training, evaluation, and deployment capabilities based on Sri Lankan labour force statistics.

## Features

- ğŸ¤– **Multiple ML Models**: Support for 6 models (XGBoost, Random Forest, Decision Tree, Gradient Boosting, Naive Bayes, Logistic Regression)
- ğŸ¯ **Modular Training Pipeline**: Complete modular training system with model comparison
- ğŸ“Š **Interactive Dashboard**: Streamlit-based web interface with model selection
- ğŸ”® **Prediction System**: Real-time employment status predictions with selected model
- ğŸ“ˆ **Model Explainability**: SHAP-based feature importance and explanations for all models
- ğŸ¨ **Visualizations**: Confusion matrix, feature importance, and SHAP plots
- ğŸ“ **Organized Structure**: Clean separation of data, models, modules, and frontend

## Project Structure

```
â”œâ”€â”€ app.py                      # Main Streamlit application entry point
â”œâ”€â”€ config.py                   # Central configuration (paths, model configs)
â”œâ”€â”€ utils.py                    # Utility functions for model loading/predictions
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ train_pipeline.py          # Complete training pipeline script
â”œâ”€â”€ PROJECT_STRUCTURE.md       # Detailed structure documentation
â”‚
â”œâ”€â”€ data/                       # Data files (organized)
â”‚   â”œâ”€â”€ labour_force_stats_sri_lanka.csv
â”‚   â”œâ”€â”€ processed_data.csv
â”‚   â”œâ”€â”€ sample_dataset.csv
â”‚   â””â”€â”€ feature_info.json
â”‚
â”œâ”€â”€ models/                     # Trained models and artifacts (organized)
â”‚   â”œâ”€â”€ model_xgboost.pkl
â”‚   â”œâ”€â”€ model_decision_tree.pkl
â”‚   â”œâ”€â”€ model_*.pkl            # Other trained models
â”‚   â”œâ”€â”€ scaler.pkl
â”‚   â”œâ”€â”€ model_info.json
â”‚   â””â”€â”€ feature_columns.json
â”‚
â”œâ”€â”€ modules/                    # Backend logic (organized)
â”‚   â”œâ”€â”€ data_preprocessing.py  # Data loading, cleaning, feature engineering
â”‚   â”œâ”€â”€ model_training.py      # Model training and tuning
â”‚   â””â”€â”€ model_evaluation.py    # Evaluation and visualization
â”‚
â”œâ”€â”€ pages/                      # Frontend pages (Streamlit convention)
â”‚   â”œâ”€â”€ dashboard.py           # Model performance dashboard
â”‚   â”œâ”€â”€ dataset.py             # Dataset exploration
â”‚   â”œâ”€â”€ train.py               # Model training interface
â”‚   â”œâ”€â”€ predict.py             # Prediction interface
â”‚   â””â”€â”€ compare_models.py      # Model comparison
â”‚
â””â”€â”€ visualizations/            # Generated plots (organized)
    â”œâ”€â”€ confusion_matrix.png
    â”œâ”€â”€ feature_importance.png
    â””â”€â”€ shap_summary.png
```

**See [PROJECT_STRUCTURE.md](PROJECT_STRUCTURE.md) for detailed documentation.**
â”‚
â”œâ”€â”€ data/                       # Data directory (created on first run)
â”‚   â””â”€â”€ winequality-red.csv    # Raw dataset
â”‚
â””â”€â”€ models/                     # Model artifacts (created during training)
    â”œâ”€â”€ model.pkl              # Trained model
    â”œâ”€â”€ scaler.pkl             # Feature scaler
    â”œâ”€â”€ confusion_matrix.png   # Performance visualizations
    â”œâ”€â”€ feature_importance.png
    â””â”€â”€ shap_summary.png
```

## Setup and Installation

### 1. Clone Repository
```bash
git clone https://github.com/yourusername/sri-lanka-employment-predictor.git
cd sri-lanka-employment-predictor
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Prepare Dataset
Place your `labour_force_stats_sri_lanka.csv` file in the `data/` directory, or use the training interface to upload your dataset.

## Usage

### Option 1: Train Model via Command Line

Train the model with default settings:
```bash
python train_pipeline.py
```

Train with custom data file:
```bash
python train_pipeline.py --data path/to/labour_force_stats_sri_lanka.csv
```

Train with hyperparameter tuning:
```bash
python train_pipeline.py --tune
```

Train with 10-fold cross-validation:
```bash
python train_pipeline.py --cv 10
```

### Option 2: Train Model via Web Interface

1. Start the Streamlit app:
```bash
streamlit run app.py
```

2. Navigate to the "Train Model" page
3. Upload your dataset or use the default path
4. Configure training options in the sidebar
5. Click "Start Training" and monitor progress

### Option 3: Use Pre-trained Model

If you already have a trained model, simply place `model.pkl` and `scaler.pkl` in the project root directory and run the Streamlit app.

## Training Pipeline Details

The training pipeline consists of three main stages:

### 1. Data Preprocessing (`modules/data_preprocessing.py`)
- Load CSV data with employment statistics
- Handle missing values and duplicates
- Feature engineering:
  - Language profile creation from language columns (SIN, ENG, TAMIL)
  - Employment status combination from Employment and Employment_2
  - Disability features aggregation and categorization
- Train-test split with stratification
- Feature binary scaling using StandardScaler

### 2. Model Training (`modules/model_training.py`)
- XGBoost classifier initialization
- Cross-validation for performance estimation
- Optional hyperparameter tuning with GridSearchCV
- Model persistence to disk

### 3. Model Evaluation (`modules/model_evaluation.py`)
- Comprehensive metrics (accuracy, precision, recall, F1)
- Confusion matrix visualization
- Feature importance analysis
- SHAP-based global and local explainability

## Streamlit Application Pagesemployment datasets
3. **Predict**: Make employment status predictions on individual record
1. **Dashboard**: View model performance metrics and visualizations
2. **Dataset**: Upload, view, and download datasets
3. **Predict**: Make predictions on individual wine samples
4. **Train Model**: Interactive model training interface

## Model Configuration

Edit `config.py` to customize:
- Data paths
- Model hyperparameters
- Feature definitions
- Training parameters

Example model parameters:
```python
MODEL_PARAMS = {
    'n_estimators': 200,
    'max_depth': 5,
    'learning_rate': 0.1,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'random_state': 42,
    'objective': 'binary:logistic'  # Binary classification
}
```

## Dataset Requirements

The CSV file should contain columns such as:
- **SECTOR, DISTRICT, PSU, SERNO**: Geographic/survey identifiers
- **SEX, AGE, MARITAL**: Demographics
- **EDU, DEGREE, CUEDU**: Education level
- **SIN, ENG, TAMIL**: Language proficiency (0/1 indicators)
- **Eye Disability, Hearing Disability, Walking Disability, Remembering Disability, Self Care Disability, Communicating Disability**: Disability indicators (1-4 scale)
- **Vocational Trained**: Vocational training status
- **Employment, Employment_2**: Employment status indicators
- **Unemployment Reason**: Reason for unemployment (if applicable)
- **Certified On Employment**: Employment certification status

Delimiter: comma (`,`)

### Feature Engineering

The preprocessing pipeline automatically creates:
- **Language_Profile_Encoded**: Combined language capabilities
- **Employment_Status_Encoded**: Binary employment status (0=Unemployed, 1=Employed)
- **Disability_Category_Encoded**: Categorized disability severity

## Development and Testing

### Test Individual Modules

Test data preprocessing:
```bash
python modules/data_preprocessing.py
```

Test model training:
```bash
python modules/model_training.py
```

Test model evaluation:
```bash
python modules/model_evaluation.py
```

### Run Streamlit Pages Independently

While the pages are designed for the multi-page app, you can debug individual pages:
```bash
streamlit run pages/train.py
```

## Deployment

### Streamlit Cloud
1. Push your code to GitHub
2. Connect your repository to Streamlit Cloud
3. Set `app.py` as the entry point
4. Deploy!

Note: Ensure model files are generated or uploaded before deployment.

### Docker task: Binary classification (Employed vs Unemployed)
- Typical accuracy: Varies by dataset characteristics and class balance
- Key features: Education level, age, language proficiency, disability status

## Performance Notes

- Expected accuracy: 0.65-0.70 on test set
- Top features: alcohol content, volatile acidity
- Training time: 1-3 minutes (without tuning)
- Tuning time: 5-15 minutes (with GridSearchCV)

## Troubleshooting

**Issue**: "File not found" errors
- Ensure dataset is in `data/` directory
- Run training pipeline to generate model files

**Issue**: Import errors in modules
- Check that you're running from project root
- Verify all dependencies are installed

**Issue**: Streamlit pages not showing
- Ensure pages are in `pages/` directory
- Check that files are named correctly (`.py` extension)

## Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

##Dataset: Sri Lankan Labour Force Statistics

MIT License - feel free to use and modify as needed.

## Acknowledgments

- Wine Quality Dataset: UCI Machine Learning Repository
- XGBoost: Gradient boosting framework
- SHAP: Model explainability library
- Streamlit: Interactive web framework