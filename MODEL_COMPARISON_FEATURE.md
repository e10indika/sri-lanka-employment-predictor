# Model Comparison Feature

## Overview

The Model Comparison feature allows you to compare all trained machine learning models side-by-side with comprehensive visualizations and metrics.

## What's Been Implemented

### 1. Backend API Endpoint

**New Endpoint:** `GET /api/models/compare`

Returns comparison data for all trained models including:
- Performance metrics (accuracy, precision, recall, F1-score)
- Cross-validation scores and statistics
- Training time
- Visualization availability status
- Best performing model identification

**Response Format:**
```json
{
  "models": [
    {
      "model_type": "xgboost",
      "model_name": "XGBoost Classifier",
      "accuracy": 0.85,
      "precision": 0.83,
      "recall": 0.87,
      "f1_score": 0.85,
      "cv_mean": 0.84,
      "cv_std": 0.02,
      "cv_scores": [0.83, 0.85, 0.84, 0.86, 0.82],
      "training_time": 5.2,
      "has_visualizations": true
    }
  ],
  "best_model": "xgboost",
  "total_models": 3
}
```

### 2. Frontend Implementation

**Location:** `frontend/src/pages/CompareModels.jsx`

**Features:**

#### A. Comparison Table
- All models displayed in a sortable table
- Metrics: Accuracy, Precision, Recall, F1 Score, CV Mean, Training Time
- Visual indicators for best performing model (green highlight + badge)
- Status chips showing visualization availability

#### B. Interactive Charts

1. **Performance Metrics Bar Chart**
   - Side-by-side comparison of all metrics
   - Color-coded bars for each metric type
   - Percentage display (0-100%)

2. **Radar Chart**
   - Multi-dimensional performance visualization
   - Shows all metrics at once for pattern recognition
   - Overlays all models for direct comparison

3. **Training Time Bar Chart**
   - Compare computational efficiency
   - Helps identify fastest models

#### C. Visual Model Comparison
- Confusion matrices displayed side-by-side
- Grid layout for easy visual comparison
- Best model badge on confusion matrix
- F1 score displayed on each card

#### D. Cross-Validation Details
- Detailed table showing CV scores for each model
- Mean and standard deviation
- Individual fold scores

### 3. API Service Integration

**Location:** `frontend/src/services/api.js`

Added new method:
```javascript
modelsAPI.compare() // Fetches comparison data
```

## How to Use

### 1. Train Multiple Models

First, train at least 2-3 different models using the "Train Model" page:
- XGBoost Classifier
- Decision Tree Classifier  
- Logistic Regression

### 2. Navigate to Compare Models

Click on "Compare Models" in the navigation menu.

### 3. View Comparisons

The page will automatically:
- Load all trained models
- Calculate comparison metrics
- Display interactive visualizations
- Highlight the best performing model

### 4. Interpret Results

**Best Model Indicators:**
- üèÜ Banner at top showing best model
- Green row highlight in comparison table
- "Best" chip badge on model name
- "Best" badge on confusion matrix

**Key Metrics to Compare:**
- **F1 Score**: Overall balanced performance (higher is better)
- **Accuracy**: Correct predictions / Total predictions
- **Precision**: Correct positive predictions / Total positive predictions
- **Recall**: Correct positive predictions / Actual positives
- **CV Mean**: Cross-validation average (model consistency)
- **Training Time**: Computational efficiency

## Technical Details

### Backend Changes

**File:** `backend/api/routes/models.py`

```python
@router.get("/compare")
async def compare_models():
    """Get comparison data for all trained models"""
    # Loads all models
    # Extracts metrics from saved model data
    # Calculates best model by F1-score
    # Returns structured comparison data
```

### Frontend Changes

**File:** `frontend/src/pages/CompareModels.jsx`

- Uses **Recharts** library for visualizations
- Material-UI for responsive layout and tables
- Real-time data fetching from API
- Error handling and loading states
- Responsive grid layout for different screen sizes

### Dependencies

**Already Installed:**
- `recharts@2.10.3` - Charting library
- `@mui/material@5.14.20` - UI components
- `axios@1.6.2` - HTTP client

## Visual Features

### 1. Color Coding
- **Blue (#8884d8)**: Accuracy
- **Green (#82ca9d)**: Precision  
- **Yellow (#ffc658)**: Recall
- **Red (#ff7c7c)**: F1 Score
- **Success Green**: Best model highlights

### 2. Layout
- Responsive grid system (12 columns)
- Mobile-friendly table (horizontal scroll)
- Card-based confusion matrix display
- Full-width charts for desktop

### 3. Interactive Elements
- Hover tooltips on charts
- Clickable legends to toggle metrics
- Table row hover effects
- Status chips with color indicators

## Best Practices

### When to Use Comparison

‚úÖ **Use comparison when:**
- You've trained multiple models
- Deciding which model to deploy
- Analyzing trade-offs (accuracy vs speed)
- Documenting model selection process

‚ùå **Don't use when:**
- Only one model is trained
- Models are still training
- Need detailed single-model analysis (use Dashboard instead)

### Interpreting Results

1. **F1 Score** is primary metric (balanced precision & recall)
2. **CV Mean** shows consistency across different data splits
3. **Training Time** matters for production deployment
4. **Radar Chart** helps spot outliers or patterns
5. **Confusion Matrices** show class-specific performance

## Future Enhancements

Potential additions:
- [ ] Export comparison report (PDF/CSV)
- [ ] Custom metric weighting for ranking
- [ ] Side-by-side feature importance comparison
- [ ] ROC curve overlay
- [ ] Statistical significance testing
- [ ] Model ensemble recommendations
- [ ] Cost-benefit analysis (time vs accuracy)

## API Testing

### Test the Comparison Endpoint

```bash
# Get all models comparison
curl http://localhost:8000/api/models/compare

# Pretty print JSON
curl http://localhost:8000/api/models/compare | python3 -m json.tool
```

### Expected Response

```json
{
  "models": [...],
  "best_model": "xgboost",
  "total_models": 3
}
```

## Troubleshooting

### "No models available for comparison"
- Train at least one model using the Train Model page
- Check that models/ directory contains .pkl files
- Verify models loaded correctly with `GET /api/models/`

### Charts not displaying
- Ensure recharts is installed: `npm install recharts`
- Check browser console for errors
- Verify API is returning data correctly

### Confusion matrices not showing
- Visualizations must be generated during training
- Re-train models if visualizations are missing
- Check visualizations/ directory for PNG files

### Backend 500 error
- Check backend logs for Python errors
- Verify model files are not corrupted
- Ensure all required fields exist in saved models

## Files Modified

1. ‚úÖ `backend/api/routes/models.py` - Added `/compare` endpoint
2. ‚úÖ `frontend/src/services/api.js` - Added `modelsAPI.compare()`
3. ‚úÖ `frontend/src/pages/CompareModels.jsx` - Complete implementation

## Testing Checklist

- [x] Backend endpoint returns correct data structure
- [x] Frontend fetches and displays data
- [x] Charts render correctly
- [x] Best model is highlighted
- [x] Responsive layout works on mobile
- [x] Error handling for no models
- [x] Loading states display properly
- [x] Confusion matrices load correctly

## Performance

- **Backend**: O(n) where n = number of models (fast)
- **Frontend**: Renders in <1s for up to 10 models
- **Charts**: Smooth animations with recharts
- **Images**: Lazy loading for confusion matrices

## Accessibility

- Semantic HTML structure
- ARIA labels on interactive elements  
- Color-blind friendly palette option
- Keyboard navigation support (Material-UI)
- Screen reader compatible tables

---

**Status:** ‚úÖ Fully Implemented  
**Version:** 1.0  
**Last Updated:** January 15, 2026
